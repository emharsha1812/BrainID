{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c086dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries and frameworks\n",
    "import tensorflow as tf\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn.preprocessing\n",
    "from scipy.signal import butter,lfilter\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54c52fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a butterworth filtter bandpass\n",
    "def butter_bandpass(lowcut,highcut,fs,order=5):\n",
    "    #Nyquist rate fs=2*fm\n",
    "    #fm=0.5*fs\n",
    "    nyq=0.5*fs\n",
    "    lowcut=lowcut/nyq\n",
    "    highcut=highcut/nyq\n",
    "    #b,a=numerator and denominator values of the IIR Filter\n",
    "    b,a=butter(order,[lowcut,highcut],btype='band')\n",
    "    return b,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c3875a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining one hot encoding\n",
    "#This function is used to transfer one column label to one hot label\n",
    "def one_hot(y_):\n",
    "    #Function to encode output labels from number indexes\n",
    "    #e.g [[5],[3],[0]] --> [[0,0,0,0,0,1],[0,0,0,1,0,0],[0,0,0,0,0,0]]\n",
    "#     y_=y_.reshape((len(y_)))\n",
    "    y_=np.array(y_)\n",
    "    y_=y_.reshape(len(y_))\n",
    "    n_values=np.max(y_)+1\n",
    "    return np.eye(int(n_values))[np.array(y_,dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60d06479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Checking\n",
    "a1=[[5],[3],[2]]\n",
    "a2=one_hot(a1)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f92dc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data,lowcut,highcut,fs,order=5):\n",
    "    b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "    y=lfilter(b,a,data)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e0cd8",
   "metadata": {},
   "source": [
    "Defining some variables used to load and process the EEG Data\n",
    "1. **len_sample** = Length of each sample ( in seconds )\n",
    "2. **full**= total number of samples per subject \n",
    "3. **n_fea**=number of features ( EEG Channels )\n",
    "4. **label0-label7** = Defines the labels for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19e9bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [7.]\n",
      " [7.]\n",
      " [7.]]\n"
     ]
    }
   ],
   "source": [
    "len_sample=1\n",
    "full=7000\n",
    "len_a=int(full/len_sample)\n",
    "label0=np.zeros(len_a)\n",
    "label1=np.ones(len_a)\n",
    "label2=np.ones(len_a)*2\n",
    "label3=np.ones(len_a)*3\n",
    "label4=np.ones(len_a)*4\n",
    "label5=np.ones(len_a)*5\n",
    "label6=np.ones(len_a)*6\n",
    "label7=np.ones(len_a)*7\n",
    "label=np.hstack((label0,label1,label2,label3,label4,label5,label6,label7))\n",
    "label=label.T\n",
    "label.shape=(len(label),1)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42c71ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start1=time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4fb6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=sc.io.loadmat(\"EID-S.mat\")\n",
    "all=feature['eeg_close_8sub_1file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5784f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fea=14\n",
    "all=all[0:full*8,0:n_fea]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7edde3f",
   "metadata": {},
   "source": [
    "### Next steps - Performing a filtering operation on a EEG Dataset to extract the delta wave pattern using a Butterworth filter\n",
    "\n",
    "1. **start2**= measures the starting time using `time.process_time()` function and stores it in a variable\n",
    "2. Initialize an empty list **data_f1** to store the filtered EEG data\n",
    "3. Loop over the columns of the `all` array using `range(all.shape[1])`\n",
    "4. For each column, applies a bandpass filter using the butter_bandpass_filter function with the specified lowcut, highcut and sampling frequency values. The filtered data is then stored in the `data_f1 list`.\n",
    "5. Convert the `data_f1` list to a numpy array and then transpose it\n",
    "6. Replace the original `all` array with the filtered data array `data_f1`\n",
    "7. Measure the ending time using the `time.process_time()` function and store it in a variable `time3`\n",
    "8. Print the shape of the filtered data array and the elapsed timem for the filtering operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20c5fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timing the process\n",
    "start=time.process_time()\n",
    "\n",
    "#Empty list\n",
    "data_f1=[]\n",
    "for i in range(all.shape[1]):\n",
    "    x=all[:,i]\n",
    "    fs=128.0\n",
    "    lowcut=0.5\n",
    "    highcut=4.0\n",
    "    y=butter_bandpass_filter(x,lowcut,highcut,fs,order=3)\n",
    "    data_f1.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf671395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_f1.shape (56000, 14)\n"
     ]
    }
   ],
   "source": [
    "data_f1=np.array(data_f1)\n",
    "data_f1=data_f1.T\n",
    "print('data_f1.shape',data_f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c85a4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PD time -0.015625\n",
      "(56000, 15)\n"
     ]
    }
   ],
   "source": [
    "all=data_f1\n",
    "end=time.process_time()\n",
    "print('PD time',start1-end)\n",
    "all=np.hstack((all,label))\n",
    "print(all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "155fa092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [7.]\n",
      " [7.]\n",
      " [7.]]\n"
     ]
    }
   ],
   "source": [
    "#all=combine data\n",
    "data_size=all.shape[0]\n",
    "\n",
    "feature_all=all[:,0:n_fea]\n",
    "print(all[:,n_fea:n_fea+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecdedd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_all=feature_all-4200 #minus Direct Current\n",
    "#z-score scaling\n",
    "feature_all=sklearn.preprocessing.scale(feature_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26ea0e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 15)\n"
     ]
    }
   ],
   "source": [
    "label_all=all[:,n_fea:n_fea+1]\n",
    "all=np.hstack((feature_all,label_all))\n",
    "print(all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b91dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the first subject as testing subject\n",
    "np.random.shuffle(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20c98111",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=all[0:int(data_size*0.875)] #Training data is 87.5% of original\n",
    "test_data=all[int(data_size*0.875):data_size]\n",
    "\n",
    "no_fea=n_fea\n",
    "n_steps=len_sample\n",
    "\n",
    "feature_training=train_data[:,0:no_fea]\n",
    "feature_training=feature_training.reshape([train_data.shape[0],n_steps,no_fea])\n",
    "\n",
    "feature_testing=test_data[:,0:no_fea]\n",
    "feature_training = feature_training.reshape([train_data.shape[0], n_steps, no_fea])\n",
    "\n",
    "\n",
    "feature_testing = test_data[:,0:no_fea]\n",
    "\n",
    "feature_testing = feature_testing.reshape([test_data.shape[0], n_steps, no_fea])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acca35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 1. 3. ... 3. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "label_training=train_data[:,no_fea]\n",
    "label_training=one_hot(label_training)\n",
    "label_testing=test_data[:,no_fea]\n",
    "print(label_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9428c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8c0c9",
   "metadata": {},
   "source": [
    "### Batch Splitting\n",
    "**Splitting the input data into batches for training in order to process it more efficiently**\n",
    "\n",
    "***Steps Involved***\n",
    "1. It calculates the batch size by multiplying the total data size by 0.125 (12.5%).\n",
    "2. It creates an empty list called train_fea.\n",
    "3. It sets the number of groups to 7.\n",
    "4. It loops through each group and selects the corresponding slice of the feature_training array to add to the train_fea list. Each slice is determined by multiplying the batch size by the group index and adding 0 to the start and end indices for the first group, adding one batch size to both the start and end indices for the second group, and so on. The result is that train_fea is a list of 7 arrays, each with batch_size rows and some number of columns.\n",
    "5. It prints the shape of the first element of train_fea (which should be (batch_size, num_features)) and the elements of rows 235 and 236 of the first element of train_fea.\n",
    "6. It creates an empty list called train_label.\n",
    "7. It loops through each group and selects the corresponding slice of the label_training array to add to the train_label list. The slices are determined in the same way as for train_fea. The result is that train_label is a list of 7 arrays, each with batch_size rows and some number of columns.\n",
    "8. It prints the shape of the first element of train_label (which should be (batch_size, num_classes)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c57264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1, 14)\n",
      "[[[ 0.08028771 -0.22681946  0.05469709  0.02443724 -0.002127\n",
      "    0.04619136  0.05424574 -0.02418474 -0.12383184 -0.01481329\n",
      "   -0.04651189  0.06176732 -0.13170864  0.04388825]]\n",
      "\n",
      " [[-0.30657352 -0.51793876 -0.16363107 -0.33376376 -0.12930287\n",
      "   -0.28284165 -0.24096585 -0.31190144 -0.24371498 -0.20759036\n",
      "   -0.08147497 -0.21883572 -0.13740341 -0.27573951]]]\n",
      "(7000, 8)\n"
     ]
    }
   ],
   "source": [
    "a=feature_training\n",
    "b=feature_testing\n",
    "nodes=30\n",
    "lameda=0.001\n",
    "lr=0.001\n",
    "\n",
    "batch_size=int(data_size*0.125)\n",
    "\n",
    "n_group=7\n",
    "\n",
    "def range_calculator(listname,listdest):\n",
    "    for i in range(n_group):\n",
    "        f=listname[(0+batch_size*i):(batch_size+batch_size*i)]\n",
    "        listdest.append(f)\n",
    "\n",
    "train_fea=[]\n",
    "train_label=[]\n",
    "\n",
    "range_calculator(a,train_fea)\n",
    "range_calculator(label_training,train_label)\n",
    "\n",
    "print(train_fea[0].shape)\n",
    "print(train_fea[0][235:237])\n",
    "\n",
    "print(train_label[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619e202",
   "metadata": {},
   "source": [
    "### Defining an RNN Cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6931d428",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to `Dense` should be defined. Found `None`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define layers\u001b[39;00m\n\u001b[0;32m     14\u001b[0m flatten \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten()(inputs)\n\u001b[1;32m---> 16\u001b[0m dense1 \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_hidden1_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdense1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m dense2 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(n_hidden2_units, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense2\u001b[39m\u001b[38;5;124m'\u001b[39m)(dense1)\n\u001b[0;32m     18\u001b[0m dense3 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(n_hidden3_units, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense3\u001b[39m\u001b[38;5;124m'\u001b[39m)(dense2)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mygpu\\lib\\site-packages\\keras\\engine\\base_layer.py:976\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 976\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mygpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1114\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1112\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[0;32m   1113\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1114\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1120\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mygpu\\lib\\site-packages\\keras\\engine\\base_layer.py:848\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 848\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mygpu\\lib\\site-packages\\keras\\engine\\base_layer.py:886\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_scope()):  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    882\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;66;03m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# overridden).\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[1;32m--> 886\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m    888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mygpu\\lib\\site-packages\\keras\\engine\\base_layer.py:2659\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_default\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   2655\u001b[0m   \u001b[38;5;66;03m# Any setup work performed only once should happen in an `init_scope`\u001b[39;00m\n\u001b[0;32m   2656\u001b[0m   \u001b[38;5;66;03m# to avoid creating symbolic Tensors that will later pollute any eager\u001b[39;00m\n\u001b[0;32m   2657\u001b[0m   \u001b[38;5;66;03m# operations.\u001b[39;00m\n\u001b[0;32m   2658\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39mmaybe_init_scope(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2659\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint:disable=not-callable\u001b[39;00m\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;66;03m# We must set also ensure that the layer is marked as built, and the build\u001b[39;00m\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;66;03m# shape is stored since user defined build functions may not be calling\u001b[39;00m\n\u001b[0;32m   2662\u001b[0m \u001b[38;5;66;03m# `super.build()`\u001b[39;00m\n\u001b[0;32m   2663\u001b[0m Layer\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m, input_shapes)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mygpu\\lib\\site-packages\\keras\\layers\\core.py:1175\u001b[0m, in \u001b[0;36mDense.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1173\u001b[0m last_dim \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mdimension_value(input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe last dimension of the inputs to `Dense` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1176\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould be defined. Found `None`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m InputSpec(min_ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m: last_dim})\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1180\u001b[0m     shape\u001b[38;5;241m=\u001b[39m[last_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1185\u001b[0m     trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: The last dimension of the inputs to `Dense` should be defined. Found `None`."
     ]
    }
   ],
   "source": [
    "n_inputs = no_fea  # MNIST data input (img shape: 11*99)\n",
    "n_hidden1_units = nodes   # neurons in hidden layer\n",
    "n_hidden2_units = nodes\n",
    "n_hidden3_units = nodes\n",
    "n_hidden4_units = nodes\n",
    "n_classes = 8  # MNIST classes (0-9 digits)\n",
    "batch_size = 128\n",
    "n_steps = None\n",
    "\n",
    "# Define input\n",
    "inputs = keras.Input(shape=(n_steps, n_inputs), name='input')\n",
    "\n",
    "# Define layers\n",
    "flatten = keras.layers.Flatten()(inputs)\n",
    "\n",
    "dense1 = keras.layers.Dense(n_hidden1_units, activation='sigmoid', name='dense1')(flatten)\n",
    "dense2 = keras.layers.Dense(n_hidden2_units, name='dense2')(dense1)\n",
    "dense3 = keras.layers.Dense(n_hidden3_units, name='dense3')(dense2)\n",
    "dense4 = keras.layers.Dense(n_hidden4_units, name='dense4')(dense3)\n",
    "\n",
    "# Define LSTM cell\n",
    "lstm_cell_1 = keras.layers.LSTM(n_hidden3_units, return_sequences=True, return_state=True, name='lstm1')\n",
    "outputs, state_h, state_c = lstm_cell_1(dense4)\n",
    "\n",
    "# Define attention model\n",
    "dense_att1 = keras.layers.Dense(n_hidden4_units, activation='tanh', name='dense_att1')(outputs)\n",
    "dense_att2 = keras.layers.Dense(1, activation='softmax', name='dense_att2')(dense_att1)\n",
    "outputs_att = keras.layers.Dot(axes=1, name='dot')([dense_att2, outputs])\n",
    "\n",
    "# Define output layer\n",
    "dense_out = keras.layers.Dense(n_classes, name='dense_out')(outputs_att)\n",
    "\n",
    "# Define model\n",
    "model = keras.Model(inputs=inputs, outputs=dense_out, name='rnn_model')\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d167978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "n_inputs=no_fea\n",
    "n_hidden1_units=nodes\n",
    "n_hidden2_units=nodes\n",
    "n_hidden3_units=nodes\n",
    "n_hidden4_units=nodes\n",
    "n_classes=8\n",
    "\n",
    "#tf graph input \n",
    "x=tf.keras.layers.Input(shape=(None,n_inputs),name=\"x\",dtype='float32')\n",
    "y=tf.keras.layers.Input(shape=(n_classes),dtype='float32',name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93586fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining weights\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random.normal([n_inputs, n_hidden1_units]), trainable=True),\n",
    "    'a': tf.Variable(tf.random.normal([n_hidden1_units, n_hidden1_units]), trainable=True),\n",
    "    'hidd2': tf.Variable(tf.random.normal([n_hidden1_units, n_hidden2_units])),\n",
    "    'hidd3': tf.Variable(tf.random.normal([n_hidden2_units, n_hidden3_units])),\n",
    "    'hidd4': tf.Variable(tf.random.normal([n_hidden3_units, n_hidden4_units])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden4_units, n_classes]), trainable=True),\n",
    "    'att': tf.Variable(tf.random.normal([n_inputs, n_hidden4_units]), trainable=True),\n",
    "    'att2': tf.Variable(tf.random.normal([1, batch_size]), trainable=True),\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden1_units])),\n",
    "    'hidd2': tf.Variable(tf.constant(0.1, shape=[n_hidden2_units])),\n",
    "    'hidd3': tf.Variable(tf.constant(0.1, shape=[n_hidden3_units])),\n",
    "    'hidd4': tf.Variable(tf.constant(0.1, shape=[n_hidden4_units])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]), trainable=True),\n",
    "    'att': tf.Variable(tf.constant(0.1, shape=[n_hidden4_units])),\n",
    "    'att2': tf.Variable(tf.constant(0.1, shape=[n_hidden4_units])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(X,weights,biases):\n",
    "    #hidden layer for input to cell\n",
    "    ##############################\n",
    "    \n",
    "    X=tf.reshape(X,[-1,n_inputs])\n",
    "    \n",
    "    #hidden layer\n",
    "    X_hidd1=tf.sigmoid(tf.matmul(X,weights['in'])+biases['in'])\n",
    "    X_hidd2=tf.matmul(X_hidd1,weights['hidd2'])+biases['hidd2']\n",
    "    X_hidd3=tf.matmul(X_hidd2,weights['hidd3'])+biases['hidd3']\n",
    "    \n",
    "    X_in=tf.reshape(X_hidd3,[-1,n_steps,n_hidden4_units])\n",
    "    \n",
    "    \n",
    "    #cell\n",
    "    #####################################\n",
    "    \n",
    "    #basic LSTM Cell\n",
    "    lstm_cell_1=tf.keras.layers.LSTMCell(n_hidden3_units,unit_forget_bias=True)\n",
    "    init_state=lstm_cell_1.get_initial_state(batch_size=batch_size,dtype=tf.float32)\n",
    "    \n",
    "    outputs,final_state=tf.keras.layers.RNN(lstm_cell_1,return_sequences=True,return_state=True)(X_in,initial_state=init_state)\n",
    "    \n",
    "    #outputs\n",
    "    outputs=tf.unstack(tf.transpose(outputs,[1,0,2])) #states is the last output\n",
    "    \n",
    "    #attention based model\n",
    "    X_att2=final_state[0] #weights\n",
    "    outputs_att=tf.multiply(outputs[-1],X_att2)\n",
    "    results=tf.matmul(outputs_att,weights['out'])+biases['out']\n",
    "    \n",
    "    return results,outputs[-1]\n",
    "\n",
    "\n",
    "pred,Feature,_=RNN(x,weights,biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamena=lameda\n",
    "#Computing l2 loss\n",
    "l2=lamena*sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))+l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.scalar_summary('loss', cost)\n",
    "\n",
    "lr=lr\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "pred_result =tf.argmax(pred, 1,name=\"pred_result\")\n",
    "label_true =tf.argmax(y, 1)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "rnn_s = time.clock()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step < 2000:\n",
    "        for i in range(n_group):\n",
    "            sess.run(train_op, feed_dict={\n",
    "                x: train_fea[i],\n",
    "                y: train_label[i],\n",
    "                })\n",
    "        #  early stopping\n",
    "        if sess.run(accuracy, feed_dict={x: b,y: label_testing,})>0.999:\n",
    "            print(\n",
    "            \"The lamda is :\", lamena, \", Learning rate:\", lr, \", The step is:\", step, \", The accuracy is: \",\n",
    "            sess.run(accuracy, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            }))\n",
    "            break\n",
    "        if step % 10 == 0:\n",
    "            pp=sess.run(pred_result,feed_dict={x: b, y: label_testing})\n",
    "            print \"predict\",pp[0:10]\n",
    "            gt=np.argmax(label_testing, 1)\n",
    "            print \"groundtruth\", gt[0:10]\n",
    "            hh = sess.run(accuracy, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            })\n",
    "            h2=sess.run(accuracy,  feed_dict={x: train_fea[i],\n",
    "                y: train_label[i]})\n",
    "            print \"training acc\", h2\n",
    "            print(\"The lamda is :\",lamena,\", Learning rate:\",lr,\", The step is:\",step,\n",
    "                  \", The accuracy is:\", hh,\", The train accuracy is:\", h2)\n",
    "            print(\"The cost is :\",sess.run(cost, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            }))\n",
    "        step += 1\n",
    "\n",
    "    endtime=time.clock()\n",
    "    B=sess.run(Feature, feed_dict={\n",
    "            x: train_fea[0],\n",
    "            y: train_label[0],\n",
    "        })\n",
    "    for i in range(1,n_group):\n",
    "        D=sess.run(Feature, feed_dict={\n",
    "            x: train_fea[i],\n",
    "            y: train_label[i],\n",
    "        })\n",
    "        B=np.vstack((B,D))\n",
    "    B = np.array(B)\n",
    "    print B.shape\n",
    "    Data_train = B  # Extracted deep features\n",
    "    Data_test = sess.run(Feature, feed_dict={x: b, y: label_testing})\n",
    "print(\"RNN run time:\", endtime-rnn_s)\n",
    "\n",
    "# XGBoost\n",
    "xgb_s=time.clock()\n",
    "xg_train = xgb.DMatrix(Data_train, label=np.argmax(label_training,1))\n",
    "xg_test = xgb.DMatrix(Data_test, label=np.argmax(label_testing,1))\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softprob' # can I replace softmax by SVM??\n",
    "# softprob produce a matrix with probability value of each class\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.7\n",
    "\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['subsample']=0.9\n",
    "param['num_class'] =n_classes\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 500\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "time8=time.clock()\n",
    "pred = bst.predict(xg_test) ;\n",
    "xgb_e=time.clock()\n",
    "print('xgb run time', xgb_e -xgb_s)\n",
    "print('RNN acc', hh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
